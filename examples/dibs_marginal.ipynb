{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e361100",
   "metadata": {},
   "source": [
    "### DiBS marginal inference of $p(G | \\mathcal{D})$ for linear Gaussian Bayes nets with BGe marginal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b52aa",
   "metadata": {},
   "source": [
    "DiBS translates the task of inferring the posterior over Bayesian networks into an inference problem over the continuous latent variable $Z$. This is achieved by modeling the directed acyclic graph $G$ of the Bayesian network using the generative model $p(G | Z)$. The prior $p(Z)$ enforces the acyclicity of $G$.\n",
    "Ultimately, this allows us to infer $p(G | \\mathcal{D})$ or $p(G, \\Theta | \\mathcal{D})$ using off-the-shelf inference methods such as Stein Variational gradient descent (SVGD) (Liu and Wang, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.random as random\n",
    "\n",
    "key = random.PRNGKey(123)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5e4a",
   "metadata": {},
   "source": [
    "### Generate synthetic ground truth Bayesian network and BN model for inference\n",
    "\n",
    "`data` contains information about and observations sampled from a synthetic, ground truth causal model with `n_vars` variables. By default, the conditional distributions are linear Gaussian. The random graph model is set by `graph_prior_str`, where `er` denotes Erdos-Renyi and `sf` scale-free graphs. \n",
    "\n",
    "`model` defines prior $p(G)$ and *marginal* likelihood $p(x | G)$ of the BN model for which DiBS will infer the posterior.\n",
    "\n",
    "**For joint posterior inference, e.g., for inference of nonlinear Bayes net models, refer to the other example notebook `dibs_joint.ipynb`.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98551a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dibs.target import make_linear_gaussian_equivalent_model\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "key, subk = random.split(key)\n",
    "data, model = make_linear_gaussian_equivalent_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "\n",
    "visualize_ground_truth(data.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e46b",
   "metadata": {},
   "source": [
    "### DiBS with SVGD\n",
    "\n",
    "Infer $p(G | D)$ under the prior and marginal likelihood defined by `model`.\n",
    "The below visualization shows the *matrix of edge probabilities* $G_\\alpha(Z^{(k)})$ implied by each transported latent particle (i.e., sample) $Z^{(k)}$ during the iterations of SVGD with DiBS. Refer to the paper for further details.\n",
    "\n",
    "To explicitly perform joint posterior inference of $p(G, \\Theta | \\mathcal{D})$ using a general likelihood $p(x | G, \\Theta)$, use the separate, analogous class `JointDiBS`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ece4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from dibs.inference import MarginalDiBS\n",
    "\n",
    "dibs = MarginalDiBS(x=data.x, interv_mask=None, inference_model=model)\n",
    "key, subk = random.split(key)\n",
    "gs = dibs.sample(key=subk, n_particles=20, steps=2000, callback_every=20, callback=dibs.visualize_callback())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dc8cc",
   "metadata": {},
   "source": [
    "### Evaluate on held-out data\n",
    "\n",
    "Form the empirical (i.e., weighted by counts) and mixture distributions (i.e., weighted by unnormalized posterior probabilities, denoted DiBS+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs_empirical = dibs.get_empirical(gs)\n",
    "dibs_mixture = dibs.get_mixture(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e275383",
   "metadata": {},
   "source": [
    "Compute some evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3dc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_marginal_likelihood\n",
    "\n",
    "for descr, dist in [('DiBS ', dibs_empirical), ('DiBS+', dibs_mixture)]:\n",
    "    \n",
    "    eshd = expected_shd(dist=dist, g=data.g)        \n",
    "    auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "    negll = neg_ave_log_marginal_likelihood(dist=dist, x=data.x_ho,\n",
    "                eltwise_log_marginal_likelihood=dibs.eltwise_log_marginal_likelihood_observ)\n",
    "    \n",
    "    print(f'{descr} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. MLL {negll:5.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88898313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
